{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>JAY BAKSHI : 50206954</strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>I've taken tweets on the topic of Valentine's day that just went by on 14th Februrary to collect tweets on.\n",
    "Below are the steps I took to get them using the Twitter Search API, TwitteR package and related structures.</h4>\n",
    "\n",
    "List of commands to run everytime you start the kernel -\n",
    "1. library inclusions/imports.\n",
    "2. Twitter OAuth call - just to make sure the connection is working for twitteR, ggmap and ggplot to work 100%.\n",
    "3. Reading .csv files into relevant data-frames and other structures (have saved an offline copies so don't need to exploit and wait up on tweets to be pulled of Search API.\n",
    "\n",
    "NOTE: Any actionable action is being shown with the markdown cell having instructions highlighted.\n",
    "\n",
    "---\n",
    "<mark><b>Run the below install packages command only once. Don't need an installation in workspace everytime.</b></mark>\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages('twitteR', repos='https://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages('roauth', repos='https://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages('ggplot2', repos='https://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages('gdata', repos='https://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "install.packages('ggmap', repos='https://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "install.packages('sqldf', repos='https://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "install.packages('geosphere', repos='http://cran.r-project.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------\n",
    "<mark><b>Library import statements.\n",
    "To be ran everytime the notebook/kernel is restarted.</b></mark>\n",
    "\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "library(twitteR)\n",
    "library(ggmap)\n",
    "library(gdata)\n",
    "library(sqldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------\n",
    "<b>Twitter Credentials.\n",
    "Will need to run this for authenticating the session everytime notebook is restarted.</b>\n",
    "<br/><mark> Rememeber about the API Limits if planning to download any tweets on a new topic or other than the data already downloaded.</mark>\n",
    "\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "appname <- \"jbcse587-2\"\n",
    "apikey <- \"WQCFmtOFZAkc22CGL6L2Yzk8f\"\n",
    "apisecret <- \"kBCux5bnsh61EmY4YQWUAJimK8ipqPOBYDvECscLBC8nTfnwXI\"\n",
    "token <- \"2573201196-BjuaKMPfcgMkYI2SVyjvoiPDdnLIaucxqbtFljU\"\n",
    "tokensecret <- \"qCsfZeIgu8f9W3QjVrjYmuVMkEFip1KAAKboFYBdaT4mU\"\n",
    "setup_twitter_oauth(apikey,apisecret,token,tokensecret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------\n",
    "<b>Twitter API search</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myTopic <- searchTwitter(\"Valentine's Day OR valentines day OR valentine OR #ValentinesDay\", lang=\"All\", n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b>Converts the Tweet data to a data-frame and displays the number of rows</b>\n",
    "<br/>Write the data to a CSV file for easy access and not needing to use the Twitter API for same repeatedly\n",
    "<br/>Use the read.csv command below for the data has already been made avaiable.\n",
    "<br/><mark> <b>myTopic.csv</b> has 20,000 tweets just downloaded in it. It can be used for additional tweets if required on the same topic\n",
    "<br/> The commands in below block also actually do need to be ran again for the data can be directly read from on the command following.</mark>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myTopic.df <- twListToDF(myTopic)\n",
    "nrow(myTopic.df)\n",
    "write.csv(myTopic.df, file=\"myTopic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "<b><mark>Load the valentines tweet data from a file.\n",
    "<br/>Use this to load up on raw tweet data.</mark></b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myTopic.df <- read.csv(file=\"myTopic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b>Finding user locations using their screenName</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scnName <- myTopic.df$screenName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(scnName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b> Convert the user list to a character vector.</b>\n",
    "<br/>The statement typeof(scnName_v) returns a character\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scnName_v <- as.vector(scnName)\n",
    "typeof(scnName_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b>Getting users.</b>\n",
    "<br/>User details are written to <b>users_df.csv</b> and this makes the user data easily accesible offline if need be.\n",
    "<br/><mark> Use the csv file created to read the data, don't need to use the TwitteR fn twListToDF everytime.</mark>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users <- lookupUsers(scnName_v, includeNA=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_df.df <- twListToDF(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write.csv(users_df, file=\"users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b>Read the users file</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user <- read.csv(\"users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b> Extracting location from the user data </b>\n",
    "<br/> Using this location, which is address in characters, I'm then extracting their geocodes i.e. <b>lat and lon</b> using ggmap package fn geocode().\n",
    "<br/> Writing down the <b>loc.csv</b> files for loc i.e. address details and also making a vector of it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc <- users_df[,c(12,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write.csv(loc,file=\"loc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc <- read.csv(file=\"loc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc.df <- as.data.frame(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc.df <- subset(loc.df, loc.df$location!=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.csv(loc.df,file=\"locdf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b>Vector helps to quickly query for the geocode and further saving it in file for offline access since the API usage is limited</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_v <- as.vector(loc.df$location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(loc_v)\n",
    "length(loc_v)\n",
    "head(loc_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write.csv(loc_v,file=\"loc_v.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "<b>Getting geocodes from Google Maps.</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testLoc<-geocode(loc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testLoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<mark>Above testLoc was as name suggests to test that the location geocoding worked fine,\n",
    "without having to try luck hitting the Google rate limit for geocoding request queries</mark>\n",
    "<br/>Splitting the location data into multiple batches of 2500 - loc1,loc2,loc3,loc4,loc5,loc6...\n",
    "<br/>loc1, 2, 3 are vectors .. the typeof command below confirms they store character vectors in them..\n",
    "<br/>I was unable to geocode all the files due to limits on Google Map service APIs, so went for 5000 locations approx, as adviced on Piazza, to get geocoded in the limited span.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrow(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc1 <- loc_v[6:2500]\n",
    "write.csv(loc1, file=\"loc1.csv\")\n",
    "\n",
    "loc2 <- loc_v[2501:5000]\n",
    "write.csv(loc2, file=\"loc2.csv\")\n",
    "\n",
    "#loc3 <- loc_v[5001:7500]\n",
    "#write.csv(loc3, file=\"loc3.csv\")\n",
    "\n",
    "#loc4 <- loc_v[7501:10000]\n",
    "#loc5 <- loc_v[10001:12500]\n",
    "#loc5 <- loc_v[12501:15000]\n",
    "#loc6 <- loc_v[15001:17500]\n",
    "#loc7 <- loc_v[17501:17878]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gloc1 <- geocode(loc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geocodeQueryCheck(userType=\"free\")\n",
    "nrow(gloc1)\n",
    "write.csv(gloc1, file=\"gloc1.csv\")\n",
    "print(\"gloc1.csv created..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gloc2 <- geocode(loc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geocodeQueryCheck(userType=\"free\")\n",
    "nrow(gloc2)\n",
    "write.csv(gloc2, file=\"gloc2.csv\")\n",
    "print(\"gloc2.csv created..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Below are the readable csv files for the geocoded locations saved above, use them for offline use. API usage is limited</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gloc1 <- read.csv(\"gloc1.csv\")\n",
    "gloc2 <- read.csv(\"gloc2.csv\")\n",
    "loc1 <- read.csv(\"loc1.csv\")\n",
    "loc2 <- read.csv(\"loc2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(loc1)\n",
    "typeof(gloc1)\n",
    "head(loc1)\n",
    "head(gloc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Combining the geocode lat and lon values with the address and saving it in a new file which contains only the valid rows with omitting the records NA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addloc1 <- merge(loc1, gloc1)\n",
    "addloc1 <- na.omit(addloc1)\n",
    "within(addloc1, rm(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write.csv(addloc1, file=\"addloc1.csv\")\n",
    "print(\"addloc1.csv is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addloc2 <- merge(loc2, gloc2)\n",
    "addloc2 <- na.omit(addloc2)\n",
    "within(addloc2, rm(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write.csv(addloc2, file=\"addloc2.csv\")\n",
    "print(\"addloc2.csv is created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b>Now we go to grouping the data based on their coordinates</b>\n",
    "<br/>I've also used the round() fn to round off the coordinates to nearby integers, this helps in grouping the nearby ones in groups/clusters that can be used to plot of the map.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addloc1 <- read.csv(\"addloc1.csv\")\n",
    "addloc2 <- read.csv(\"addloc2.csv\")\n",
    "\n",
    "addloc1$lon <- round(addloc1$lon)\n",
    "addloc1$lat <- round(addloc1$lat)\n",
    "addloc2$lon <- round(addloc2$lon)\n",
    "addloc2$lat <- round(addloc2$lat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Just checking up on the structures just created/addressed to.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(addloc1)\n",
    "typeof(addloc2)\n",
    "head(addloc1)\n",
    "head(addloc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Combining the 2 data in seperated lists in to a single on addloc_full that will contain 5000 location data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addloc_full <- rbind(addloc1, addloc2)\n",
    "head(addloc_full)\n",
    "nrow(addloc1)\n",
    "nrow(addloc2)\n",
    "nrow(addloc_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Writing down the processible location data to be used to plot on the map in <b>addloc_full.csv</b> file.\n",
    "<br/>This can be then directly used later on to quickly move to plotting them on the map in the following commands.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write.csv(addloc_full, file=\"addloc_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b><mark>\n",
    "Use the below read command to access all the 5000 location data, which is further aggregated a bit below to allow us to rightfully plot the data points in the respective groups that they belong to.\n",
    "</mark></b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addloc_full <- read.csv(file=\"addloc_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_data <- aggregate(addloc_full, by =list(addloc_full$lon, addloc_full$lat), FUN=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<b> Now we'll start plotting these points on a imported google map </b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usaGeoCenter = as.numeric(geocode(\"United States\"))\n",
    "usaGeoCenter\n",
    "myUsaMap = get_map(location= usaGeoCenter, zoom=4, maptype=\"roadmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggmap(myUsaMap) + geom_point(aes(x=grp_data$Group.1, y=grp_data$Group.2),\n",
    "                             data = grp_data, alpha = .5, color=\"darkred\",\n",
    "                             size=sqrt(grp_data$address), na.rm=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<mark>Citations -\n",
    "1. https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/ggmap/ggmapCheatsheet.pdf helped me better way to plot and more accurate commands to be used.\n",
    "2. The R Manual has been in constant use to find the right functions to use https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\n",
    "3. Statistical Data Analysis using R site has been immensely helpful in understanding the functions and structure in R with their examples in addition to R Manual https://stat.ethz.ch/R-manual/R-devel/doc/html/\n",
    "4. ggmap package information on usage and available functions https://cran.r-project.org/web/packages/ggmap/ggmap.pdf\n",
    "5. ggplot package information on usage and available functions https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf and http://docs.ggplot2.org/current/geom_point.html\n",
    "6. twitteR package information on usage and available functions https://cran.r-project.org/web/packages/twitteR/twitteR.pdf\n",
    "7. The markdown cells usage and options have been referred to from http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "8. A refresher on HTML tags https://www.w3schools.com/tags/tag_mark.asp\n",
    "9. To understand how grouping can benefit I referred to understand from the examples given here http://stats.stackexchange.com/questions/8225/how-to-summarize-data-by-group-in-r , although no code or package was used but to understand the idea of grouping helps in getting summarization too.\n",
    "10. Round function usage information was understood and referred from here https://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html\n",
    "\n",
    "\n",
    "</mark>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
